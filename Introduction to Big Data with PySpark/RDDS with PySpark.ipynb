{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDDs are the building blocks of Spark. It is useful for processing unstructured data, such as text or images. \n",
    "\n",
    "### RDDs has three Main properties:\n",
    "\n",
    "1. **Resilient**: RDDs are fault-tolerant, meaning that you can recompute lost data due to node failures.\n",
    "2. **Distributed**: RDDs are distributed across multiple nodes in a cluster.\n",
    "3. **Operated in parallel**: RDDs can be operated in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Coding with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SparkSession`\n",
    "\n",
    "It is the entry point to Spark SQL. It is used to create DataFrame, register DataFrame as tables and execute SQL over tables, read parquet files, etc.\n",
    "\n",
    "#### `sparkContext`\n",
    "`sparkContext` within `SparkSession` is the connection to the Spark cluster and can be used to create and transform RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sparkContext.parallelize()`\n",
    "\n",
    "Used to create an RDD from data saved locally. We can add an argument to specify the number of partitions to split the data into. Spark defaults to the number of cores on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default setting\n",
    "rdd_par = spark.sparkContext.parallelize(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sparkContext.textFile()`\n",
    "\n",
    "Used to create an RDD from a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with partition argument of 10\n",
    "rdd_txt = spark.sparkContext.textFile(\"file_name.txt\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `getNumPartitions()`\n",
    "\n",
    "Used to verify the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_txt.getNumPartitions()\n",
    "# output: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stop()`\n",
    "\n",
    "Used to stop the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `map()`\n",
    "\n",
    "applies a function to each element in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.SparkContent.parallelize([1,2,3,4,5])\n",
    "rdd.map(lambda x: x+1)\n",
    "# output RDD [2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the RDD contains tuples, we can map the lambda expression to the elements with the specific index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input RDD [(1,2,3),(4,5,6),(7,8,9)]\n",
    "rdd.map(lambda x: (x[0]+1, x[1], x[2]))\n",
    "# output RDD [(2,2,3),(5,5,6),(8,8,9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be used to create a new RDD by selecting specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_gpa = rdd.map(lambda x: x[2]).reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `filter()`\n",
    "\n",
    "applies a function to each element in the RDD and returns elements that satisfy the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input RDD [1,2,NULL,4,5]\n",
    "rdd.filter(lambda x: x is not None)\n",
    "# output RDD [1,2,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark  Transformation Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `collect()`\n",
    "\n",
    "returns all elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda x: x is not None).collect()\n",
    "# output [1,2,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark execute the transformations only when an **action** is called. Spark transforms the data in a **lazy** manner. In pandas, the data is transformed immediately or **eagerly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.SparkContent.parallelize([1,2,3,4,5])\n",
    "rdd.map(lambda x: x+1).filter(lambda x: x>3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of following the order that we called the transformations, Spark will optimize the transformations to reduce overhead. Spark might load the values greater than 3 first and perform the map function later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `take()` \n",
    "\n",
    "returns the first n elements in the RDD. It is useful for debugging and much preferable to `collect()` since collect returns all elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input RDD [1,2,3,4,5]\n",
    "rdd.take(3)\n",
    "# output [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `reduce()`\n",
    "\n",
    "applies a function to each element in the RDD and returns a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet uses the `reduce()` function to sum all the elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input RDD [1,2,3,4,5]\n",
    "rdd.reduce(lambda x,y: x+y)\n",
    "# output 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `count()`\n",
    "returns the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input RDD [1,2,3,4,5]\n",
    "rdd.count()\n",
    "# output 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative and Commutative Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce()` is a powerful aggregation function but it requires the function to be associative and commutative due to the distributed nature of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [1,2,3,4,5]\n",
    "for i in range(1,5):\n",
    "    rdd = spark.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('addition: ', rdd.reduce(lambda a,b: a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "partition:  [[1, 2, 3, 4, 5]]\n",
    "addition:  15\n",
    "partition:  [[1, 2], [3, 4, 5]]\n",
    "addition:  15\n",
    "partition:  [[1], [2, 3], [4, 5]]\n",
    "addition:  15\n",
    "partition:  [[1], [2], [3], [4, 5]]\n",
    "addition:  15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    rdd = spark.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('division: ', rdd.reduce(lambda a,b: a/b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "partition:  [[1, 2, 3, 4, 5]]\n",
    "division:  0.008333333333333333\n",
    "partition:  [[1, 2], [3, 4, 5]]\n",
    "division:  3.3333333333333335\n",
    "partition:  [[1], [2, 3], [4, 5]]\n",
    "division:  1.875\n",
    "partition:  [[1], [2], [3], [4, 5]]\n",
    "division:  0.20833333333333331\n",
    "```\n",
    "\n",
    "Notice in the output that no matter how our list is being partitioned, the sum is still 15, but the division operation has different solutions based on the partitioning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
