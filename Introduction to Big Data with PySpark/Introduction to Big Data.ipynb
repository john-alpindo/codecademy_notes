{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 3 Vs of Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume\n",
    "\n",
    "Volume refers to the amount of data that is generated. The data is generated from different sources like business transactions, social media, sensors, mobile devices, etc. The data generated is huge and is in the order of terabytes and petabytes.\n",
    "\n",
    "## Velocity\n",
    "\n",
    "Velocity refers to the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. The data is streamed in real-time and is processed in a timely manner.\n",
    "\n",
    "## Variety\n",
    "\n",
    "Variety refers to the different types of data that are generated. The data can be in the form of structured, unstructured, and semi-structured data. The data can be in the form of text, images, videos, etc. The data is generated from different sources and is in different formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Storage and Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Storage\n",
    "\n",
    "A **cluster** in the context of big data refers to a group of interconnected computers or servers that work together to process and store large amounts of data. Each individual computer or server in the cluster is called a **node**. The **cluster manager** is responsible for coordinating and managing the resources and tasks within the cluster. It assigns tasks to the **worker nodes**, which are the nodes that actually perform the data processing and storage operations. The worker nodes execute the tasks assigned to them by the cluster manager, allowing for parallel processing and distributed storage of big data. This distributed architecture enables efficient and scalable processing of large datasets by dividing the workload among multiple nodes in the cluster.\n",
    "\n",
    "#### Hadoop Distributed File System (HDFS)\n",
    "\n",
    "The Hadoop Distributed File System (HDFS), part of Apache's toolset, is a popular framework for cluster systems designed to store large data sets for processing with MapReduce. Implementing HDFS requires specific, often costly hardware, which can be a barrier for many companies. To address this, cloud-based HDFS solutions from Microsoft Azure and Amazon Web Services (AWS) are available, allowing businesses to outsource setup and hardware management for a fixed monthly fee.\n",
    "\n",
    "HDFS stores and processes data on each worker node, providing sufficient computing power for data problems. As data size increases, more nodes can be added for additional storage and computing power, which aids scalability but can be costly as the node count rises.\n",
    "\n",
    "#### Object Storage\n",
    "\n",
    "A rapidly growing type of distributed file system is object storage, which separates storage from computing power. This framework allows any computing power or framework to be used with the stored data. Cloud providers like Microsoft Azure, Amazon Web Services (AWS), and Google Cloud offer object storage layers for storing various file types and datasets.\n",
    "\n",
    "Object storage layers have a lower barrier to entry and greater flexibility compared to HDFS. They support multiple file formats, including CSV, Parquet, and high-performance formats like Delta and Iceberg. This separation enables independent scaling of storage and computing power, improving efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Computing\n",
    "\n",
    "### MapReduce\n",
    "\n",
    "MapReduce is a programming model and processing framework for distributed computing on large data sets. It is designed to scale across hundreds or thousands of servers, each offering local computation and storage. MapReduce consists of two main phases: the Map phase and the Reduce phase.\n",
    "\n",
    "MapReduce was once the standard for big data processing, but it struggled to keep pace with the rapid growth and evolution of data. Apache Spark eventually emerged as a superior alternative, primarily because it processes data in memory rather than on disk, as MapReduce does. This in-memory processing significantly improved performance and enabled new possibilities for big data tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
