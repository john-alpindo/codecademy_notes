{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark DataFrames\n",
    "\n",
    "PySpark SQL DataFrame is a distributed collection of data organized into named columns. Under the hood, DataFrames are built on top of RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rdd.toDF()`\n",
    "The `toDF()` method is used to convert an RDD to DataFrame. The method is available on RDD of Row objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list\n",
    "hrly_views_rdd  = spark.sparkContext.parallelize([\n",
    "    [\"Betty_White\" , 288886],\n",
    "    [\"Main_Page\", 139564],\n",
    "    [\"New_Year's_Day\", 7892],\n",
    "    [\"ABBA\", 8154]\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "hrly_views_df = hrly_views_rdd\\\n",
    "    .toDF([\"article_title\", \"view_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.show()`\n",
    "\n",
    "The `show()` method is used to display the content of the DataFrame. By default, it shows the first 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+--------------+-----------+\n",
    "| article_title| view_count|\n",
    "+--------------+-----------+\n",
    "|   Betty_White|     288886|\n",
    "|     Main_Page|     139564|\n",
    "|New_Year's_Day|       7892|\n",
    "|          ABBA|       8154|\n",
    "+--------------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.rdd`\n",
    "\n",
    "The `rdd` attribute is used to convert a DataFrame to RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access DataFrame's underlying RDD\n",
    "hrly_views_df_rdd = hrly_views_df.rdd\n",
    "\n",
    "# Check object type\n",
    "print(type(hrly_views_df_rdd)) \n",
    "# <class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames from Exernal Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(spark.read)) \n",
    "# <class 'pyspark.sql.readwriter.DataFrameReader'>\n",
    "\n",
    "# Read CSV to DataFrame\n",
    "hrly_views_df = spark.read\\\n",
    ".option('header', True) \\\n",
    ".option('delimiter', ' ') \\\n",
    ".option('inferSchema', True)\\ \n",
    ".csv('views_2022_01_01_000000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting and Cleaning Data with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.printSchema()`\n",
    "\n",
    "The `printSchema()` method is used to print the schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame schema\n",
    "hrly_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "root\n",
    "|-- language_code: string (nullable = true)\n",
    "|-- article_title: string (nullable = true)\n",
    "|-- hourly_count: integer (nullable = true)\n",
    "|-- monthly_count: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.describe()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `describe()` method to get the summary statistics of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df_desc = hrly_views_df.describe()\n",
    "hrly_views_df_desc.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------+-------------+-------------+------------+-------------+\n",
    "|summary|language_code|article_title|hourly_count|monthly_count|\n",
    "+-------+-------------+-------------+------------+-------------+\n",
    "|  count|      4654091|      4654091|     4654091|      4654091|\n",
    "|   mean|         null|         null|     4.52417|          0.0|\n",
    "| stddev|         null|         null|   182.92502|          0.0|\n",
    "|    min|           aa|            -|           1|            0|\n",
    "|    max|       zu.m.d|            -|      288886|            0|\n",
    "+-------+-------------+-------------+------------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataframe.drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `monthly_count` and display new DataFrame\n",
    "hrly_views_df = hrly_views_df.drop('monthly_count')\n",
    "hrly_views_df.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+---------------------------+------------+\n",
    "|language_code|article_title              |hourly_count|\n",
    "+-------------+---------------------------+------------+\n",
    "|en           |Cividade_de_Terroso        |           2|\n",
    "|en           |Peel_Session_(Autechre_EP) |           2|\n",
    "|en           |Young_Street_Bridge        |           1|\n",
    "|en           |Troy,_Alabama              |           1|\n",
    "|en           |Charlotte_Johnson_Wahl     |          10|\n",
    "+-------------+---------------------------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.withColumnRenamed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df = hrly_views_df\\\n",
    ".withColumnRenamed('article_title', 'page_title')\n",
    "# Display DataFrame schema\n",
    "hrly_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "root\n",
    "|-- language_code: string (nullable = true)\n",
    "|-- page_title: string (nullable = true)\n",
    "|-- hourly_count: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying PySpark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.filter()`\n",
    "\n",
    "It is used to filter the rows of the DataFrame based on the given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df\\\n",
    "    .filter(hrly_views_df.language_code == \"kw.m\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "|language_code|article_title          |hourly_count|monthly_count|\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "|kw.m         |Bresel_Diabarth_Spayn  |1           |0                  |\n",
    "|kw.m         |Can_an_Pescador_Kernûak|1           |0                  |\n",
    "|kw.m         |Ferdinand_Magellan     |1           |0                  |\n",
    "|kw.m         |Justė_Arlauskaitė      |16          |0                  |\n",
    "|kw.m         |Lithouani              |2           |0                  |\n",
    "|kw.m         |Nolwenn_Leroy          |1           |0                  |\n",
    "|kw.m         |Ohio                   |1           |0                  |\n",
    "|kw.m         |Taywan                 |1           |0                  |\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.select()`\n",
    "\n",
    "It is used to select the columns of the DataFrame. Analogous to the SQL `SELECT` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.orderBy()`\n",
    "\n",
    "It is used to sort the DataFrame based on the given column. Analogous to the SQL `ORDER BY` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df\\\n",
    "    .filter(hrly_views_df.language_code == \"kw.m\")\\\n",
    "    .select(['language_code', 'article_title', 'hourly_count'])\\\n",
    "    .orderBy('hourly_count', ascending=False)\\    \n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "|language_code|article_title          |hourly_count|total_monthly_count|\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "|kw.m         |Justė_Arlauskaitė      |16          |0                  |\n",
    "|kw.m         |Lithouani              |2           |0                  |\n",
    "|kw.m         |Bresel_Diabarth_Spayn  |1           |0                  |\n",
    "|kw.m         |Can_an_Pescador_Kernûak|1           |0                  |\n",
    "|kw.m         |Nolwenn_Leroy          |1           |0                  |\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.groupBy()`\n",
    "\n",
    "It is used to group the DataFrame based on the given column. Analogous to the SQL `GROUP BY` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the sum of `hourly_count` by `language_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df\\\n",
    "    .select(['language_code', 'hourly_count'])\\\n",
    "    .groupBy('language_code')\\\n",
    "    .sum() \\\n",
    "    .orderBy('sum(hourly_count)', ascending=False)\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+-----------------+\n",
    "|language_code|sum(hourly_count)|\n",
    "+-------------+-----------------+\n",
    "|en.m         |8095763          |\n",
    "|en           |2693185          |\n",
    "|de.m         |1313505          |\n",
    "|es.m         |963835           |\n",
    "|ru.m         |927583           |\n",
    "+-------------+-----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying PySpark with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.createOrReplaceTempView()`\n",
    "\n",
    "It is used to create a temporary view of the DataFrame. The temporary view can be queried using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df.createOrReplaceTempView('hourly_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SparkSession.sql()`\n",
    "\n",
    "The `sql()` method is used to execute SQL queries on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM hourly_counts WHERE language_code = 'kw.m'\"\"\"\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+-----------------------+------------+-------------+\n",
    "|language_code|article_title          |hourly_count|monthly_count|\n",
    "+-------------+-----------------------+------------+-------------+\n",
    "|kw.m         |Bresel_Diabarth_Spayn  |           1|            0|\n",
    "|kw.m         |Can_an_Pescador_Kernûak|           1|            0|\n",
    "|kw.m         |Ferdinand_Magellan     |           1|            0|\n",
    "|kw.m         |Justė_Arlauskaitė      |          16|            0|\n",
    "|kw.m         |Lithouani              |           2|            0|\n",
    "|kw.m         |Nolwenn_Leroy          |           1|            0|\n",
    "|kw.m         |Ohio                   |           1|            0|\n",
    "|kw.m         |Taywan                 |           1|            0|\n",
    "+-------------+-----------------------+------------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT language_code, article_title, hourly_count\n",
    "    FROM hourly_counts\n",
    "    WHERE language_code = 'kw.m'\n",
    "    ORDER BY hourly_count DESC\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "|language_code|article_title          |hourly_count|total_monthly_count|\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "|kw.m         |Justė_Arlauskaitė      |          16|                  0|\n",
    "|kw.m         |Lithouani              |           2|                  0|\n",
    "|kw.m         |Bresel_Diabarth_Spayn  |           1|                  0|\n",
    "|kw.m         |Can_an_Pescador_Kernûak|           1|                  0|\n",
    "|kw.m         |Nolwenn_Leroy          |           1|                  0|\n",
    "+-------------+-----------------------+------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT language_code, SUM(hourly_count) as sum_hourly_count\n",
    "    FROM hourly_counts\n",
    "    GROUP BY language_code\n",
    "    ORDER BY sum_hourly_count DESC\"\"\"\n",
    "\n",
    "spark.sql(query).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------+-----------------+\n",
    "|language_code|sum(hourly_count)|\n",
    "+-------------+-----------------+\n",
    "|en.m         |8095763          |\n",
    "|en           |2693185          |\n",
    "|de.m         |1313505          |\n",
    "|es.m         |963835           |\n",
    "|ru.m         |927583           |\n",
    "+-------------+-----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving PySpark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SparkSession.write()`\n",
    "\n",
    "The `write()` method is used to save the DataFrame to a file. The `select()` method is used to select the columns to save. The `mode` parameter is used to specify the behavior when the file already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df\\\n",
    "    .select(['language_code', 'article_title', 'hourly_count'])\\\n",
    "    .write.csv('cleaned/csv/views_2022_01_01_000000/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `SparkSession.read()` to confirm that it looks the same as the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame back from disk\n",
    "hrly_views_df_restored = spark.read\\\n",
    "    .csv('cleaned/csv/views_2022_01_01_000000/')\n",
    "hrly_views_df_restored.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "root\n",
    "|-- _c0: string (nullable = true)\n",
    "|-- _c1: string (nullable = true)\n",
    "|-- _c2: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV do not retain information about column headers or datatypes. The file format called `Parquet` offers efficient storage and encoding of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet\n",
    "hrly_views_slim_df\n",
    "    .write.parquet('cleaned/parquet/views_2022_01_01_000000/', mode=\"overwrite\")\n",
    "\n",
    "# Read Parquet as DataFrame\n",
    "hrly_views_df_restored = spark.read\\\n",
    "    .parquet('cleaned/parquet/views_2022_01_01_000000/')\n",
    "\n",
    "# Check DataFrame's schema\n",
    "hrly_views_df_restored.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "root\n",
    "|-- language_code: string (nullable = true)\n",
    "|-- article_title: string (nullable = true)\n",
    "|-- hourly_count: integer (nullable = true)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
